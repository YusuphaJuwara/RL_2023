{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ils1pjBkthfF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ils1pjBkthfF",
        "outputId": "7a749da6-d2e4-4ca9-aeab-496517ec0d18"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6251a15f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "6251a15f",
        "outputId": "4c93a197-8f44-40ef-e595-83504b0653e0"
      },
      "outputs": [],
      "source": [
        "###import\n",
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import time\n",
        "from gymnasium import spaces\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from statistics import mean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1585649",
      "metadata": {
        "id": "d1585649"
      },
      "source": [
        "## Grid world environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e6d16f",
      "metadata": {
        "id": "26e6d16f"
      },
      "outputs": [],
      "source": [
        "####################################### GRID WORLD ENV\n",
        "# custom 2d grid world enviroment\n",
        "class GridWorld(gym.Env):\n",
        "    metadata = {'render.modes': ['console']}\n",
        "\n",
        "    # actions available\n",
        "    UP = 0\n",
        "    LEFT = 1\n",
        "    DOWN = 2\n",
        "    RIGHT = 3\n",
        "\n",
        "\n",
        "    def __init__(self, width, height, reward_type=\"sparse\", obstacles=False):\n",
        "        super(GridWorld, self).__init__()\n",
        "        self.ACTION_NAMES = [\"UP\", \"LEFT\", \"DOWN\", \"RIGHT\"]\n",
        "        self.num_actions = 4\n",
        "\n",
        "        self.size = width * height  # size of the grid world\n",
        "        self.num_states = self.size\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        if obstacles:\n",
        "            self.num_obstacles = int((width+height)/2)\n",
        "        else:\n",
        "            self.num_obstacles = 0\n",
        "        self.end_state = np.array([height - 1, width - 1], dtype=np.uint8) # goal state = bottom right cell\n",
        "\n",
        "        # actions of agents : up, down, left and right\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        # observation : cell indices in the grid\n",
        "        self.observation_space = spaces.MultiDiscrete([self.height, self.width])\n",
        "\n",
        "        self.obstacles = np.zeros((height, width))\n",
        "\n",
        "        for i in range(self.num_obstacles):\n",
        "            self.obstacles[ random.randrange(height) , random.randrange(width)] = 1\n",
        "\n",
        "        self.num_steps = 0\n",
        "        self.max_steps = height*width\n",
        "\n",
        "        self.current_state = np.zeros((2), np.uint8)#init state = [0,0]\n",
        "\n",
        "        self.directions = np.array([\n",
        "            [-1,0], #UP\n",
        "            [0,-1], #LEFT\n",
        "            [1,0], #DOWN\n",
        "            [0,1] #RIGHT\n",
        "        ])\n",
        "\n",
        "        self.reward_type= reward_type\n",
        "\n",
        "    def transition_function(self, s, a):\n",
        "        s_prime =  np.zeros((2), np.uint8)\n",
        "        s_prime = s + self.directions[a,:]\n",
        "\n",
        "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
        "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
        "                return s_prime\n",
        "\n",
        "        return s\n",
        "\n",
        "\n",
        "    def reward_function(self,s):\n",
        "\n",
        "        r = -1\n",
        "\n",
        "        if self.reward_type == \"sparse\":\n",
        "            if (s == self.end_state).all():\n",
        "                r = 100\n",
        "        else:\n",
        "            r = 1 / (np.sum((self.end_state - s))+0.000001)\n",
        "\n",
        "        return r\n",
        "\n",
        "    def termination_condition(self, s):\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        terminated = (s == self.end_state).all()\n",
        "        truncated = self.num_steps > self.max_steps\n",
        "\n",
        "        return terminated, truncated\n",
        "\n",
        "    def step(self, action):\n",
        "        s_prime = self.transition_function(self.current_state, action)\n",
        "        reward = self.reward_function(s_prime)\n",
        "        terminated, truncated = self.termination_condition(s_prime)\n",
        "\n",
        "        self.current_state = s_prime\n",
        "        self.num_steps += 1\n",
        "\n",
        "        return self.current_state, reward, terminated, truncated, None\n",
        "\n",
        "    def render(self):\n",
        "        '''\n",
        "            render the state\n",
        "        '''\n",
        "\n",
        "        row = self.current_state[0]\n",
        "        col = self.current_state[1]\n",
        "\n",
        "        for r in range(self.height):\n",
        "            for c in range(self.width):\n",
        "                if r == row and c == col:\n",
        "                    print(\"| A \", end='')\n",
        "                elif r == self.end_state[0] and c == self.end_state[1]:\n",
        "                    print(\"| G \", end='')\n",
        "                else:\n",
        "                    if self.obstacles[r,c] == 1:\n",
        "                        print('|///', end='')\n",
        "                    else:\n",
        "                        print('|___', end='')\n",
        "            print('|')\n",
        "        print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = np.zeros((2), np.uint8)\n",
        "        self.num_steps = 0\n",
        "\n",
        "        return self.current_state\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class NonDeterministicGridWorld(GridWorld):\n",
        "    def __init__(self, width, height, p=0.8, reward_type=\"sparse\", obstacles=False):\n",
        "        super(NonDeterministicGridWorld, self).__init__(width, height, reward_type, obstacles)\n",
        "        self.probability_right_action = p\n",
        "\n",
        "    def transition_function(self, s, a):\n",
        "        s_prime = s + self.directions[a, :]\n",
        "\n",
        "        #with probability 1 - p diagonal movement\n",
        "        if random.random() <= 1 - self.probability_right_action:\n",
        "            if random.random() < 0.5:\n",
        "                s_prime = s_prime + self.directions[(a+1)%self.num_actions, :]\n",
        "            else:\n",
        "                s_prime = s_prime + self.directions[(a-1)%self.num_actions, :]\n",
        "\n",
        "\n",
        "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
        "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
        "                return s_prime\n",
        "\n",
        "        return s\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82a57d26",
      "metadata": {
        "id": "82a57d26"
      },
      "source": [
        "# Q learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd1fe50",
      "metadata": {
        "id": "3dd1fe50"
      },
      "source": [
        "## Q function evluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ca7f47",
      "metadata": {
        "id": "d7ca7f47"
      },
      "outputs": [],
      "source": [
        "def evaluate_q_table(env, q_table, n_episodes=10, render=False):\n",
        "    cum_rews = []\n",
        "    for ep in range(n_episodes):\n",
        "        if render:\n",
        "            print(\"################## Episode \",ep+1)\n",
        "        cum_rew = 0\n",
        "        done = False\n",
        "        state = env.reset()\n",
        "        while not done:\n",
        "            action = np.argmax(q_table[state[0], state[1]]) # Exploit learned values\n",
        "            ############## simulate the action\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            if render:\n",
        "                print(env.ACTION_NAMES[action])\n",
        "                env.render()\n",
        "                print(done)\n",
        "            cum_rew += reward\n",
        "        cum_rews.append(cum_rew)\n",
        "    mean_rew = mean(cum_rews)\n",
        "    if render:\n",
        "        print(\"Mean reward obtained in {} episodes: {} \".format( n_episodes, mean_rew))\n",
        "    return mean_rew\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e71b8c93",
      "metadata": {
        "id": "e71b8c93"
      },
      "source": [
        "## Q learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07a47ce",
      "metadata": {
        "id": "b07a47ce"
      },
      "outputs": [],
      "source": [
        "def q_learning(env, alpha=1.0, gamma=0.99, initial_epsilon=1.0, n_episodes=10000):\n",
        "    ####### Hyperparameters\n",
        "    # alpha = learning rate\n",
        "    # gamma = discount factor\n",
        "    # initial_epsilon = initial epsilon value\n",
        "    # n_episodes = number of episodes\n",
        "\n",
        "    ############# define Q table and initialize to zero\n",
        "    # Q size = (state_var1_size , ..., state_varN_size, action_size)\n",
        "    q_table_size = []\n",
        "    for size in env.observation_space.nvec:\n",
        "        q_table_size.append(size)\n",
        "\n",
        "    q_table_size.append(env.action_space.n)\n",
        "    print(\"Q table size: \", q_table_size)\n",
        "\n",
        "    q_table = np.zeros(q_table_size)\n",
        "\n",
        "    # init epsilon\n",
        "    epsilon = initial_epsilon\n",
        "\n",
        "    received_first_reward = False\n",
        "\n",
        "    #evaluation\n",
        "    evaluation_rewards = []\n",
        "    mean_evaluation_rewards = []\n",
        "    window = 100\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # TODO choose the action with epsilon-greedy strategy\n",
        "            # (you can use random.random() to generate a random number\n",
        "            # between 0 and 1)\n",
        "            action = ...\n",
        "\n",
        "            ############## simulate the action\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            #env.render()\n",
        "\n",
        "            # TODO update q table\n",
        "            q_table[state[0],state[1], action] = ...\n",
        "\n",
        "            if not received_first_reward and reward > 0:\n",
        "                received_first_reward = True\n",
        "                print(\"Received first reward at episode \", ep)\n",
        "            #update current state\n",
        "            state = next_state\n",
        "\n",
        "        #update current epsilon\n",
        "        if received_first_reward:\n",
        "            epsilon= 0.9999*epsilon\n",
        "\n",
        "        #evaluate policy\n",
        "        evaluation_rewards.append( evaluate_q_table(env, q_table, n_episodes=5) )\n",
        "        mean_evaluation_rewards.append(mean(evaluation_rewards[-window: ]))\n",
        "\n",
        "\n",
        "    plt.plot(mean_evaluation_rewards)\n",
        "\n",
        "    print(\"Training finished in {} episodes\\n\".format(n_episodes))\n",
        "    return q_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2123e9d0",
      "metadata": {
        "id": "2123e9d0"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1995f742",
      "metadata": {
        "id": "1995f742"
      },
      "outputs": [],
      "source": [
        "#define the env\n",
        "env = NonDeterministicGridWorld(5,5, reward_type=\"sparse\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7762ff15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7762ff15",
        "outputId": "9f5e4f3e-88e3-4ee3-a01a-65faf8dba649"
      },
      "outputs": [],
      "source": [
        "# random exploration\n",
        "done = False\n",
        "env.reset()\n",
        "while not done:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "\n",
        "            print(env.ACTION_NAMES[action])\n",
        "            ############## simulate the action\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            print(done)\n",
        "            print(reward)\n",
        "            env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8edea7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "5d8edea7",
        "outputId": "03e742ff-828a-4bb0-b16d-f4b1bcfcdb26"
      },
      "outputs": [],
      "source": [
        "## Q learning\n",
        "q_table = q_learning(env, alpha=0.2, initial_epsilon=0.8, n_episodes=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce3092d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ce3092d",
        "outputId": "d422667b-11ad-44c2-ec18-2f48dd349315"
      },
      "outputs": [],
      "source": [
        "#print q values\n",
        "for action in range(env.action_space.n):\n",
        "    print(\"Q(s, a={})\".format(action))\n",
        "    print(q_table[:,:,action])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5c5160e",
      "metadata": {
        "id": "a5c5160e"
      },
      "source": [
        "## Evaluate policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a037ade3",
      "metadata": {
        "id": "a037ade3"
      },
      "source": [
        "## Render the agent behaviour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df1e9d0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df1e9d0a",
        "outputId": "3187b6c4-522d-436d-def1-e26df7ef8e18"
      },
      "outputs": [],
      "source": [
        "mean_cum_rew = evaluate_q_table(env, q_table, render=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25e939e",
      "metadata": {
        "id": "d25e939e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7942f6d2",
      "metadata": {
        "id": "7942f6d2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
