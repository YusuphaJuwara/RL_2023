{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VjX_2mvrN7c1",
        "outputId": "739f2177-ada7-4f2b-8166-0151790723ae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_nqTKTwODqj",
        "outputId": "b7d99f31-d263-4fca-f4be-5757ca29b61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Aq405erfpGKv"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import pickle\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raB0pq_vuaak",
        "outputId": "58b2e7f3-6224-40ee-ced3-ef6bd1a42f61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x5Y4Y3eXugTc"
      },
      "outputs": [],
      "source": [
        "env_id = \"CarRacing-v2\"\n",
        "\n",
        "# Create the env\n",
        "env = gym.make(env_id, continuous=False, domain_randomize=False)\n",
        "\n",
        "# Create the evaluation env\n",
        "eval_env = gym.make(env_id, continuous=False, domain_randomize=False, render_mode=\"human\")\n",
        "\n",
        "# Get the state space and action space\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "n_frames = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NpSZGAuKulI6"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, n_frames, n_actions, hidden_size, img_size=(64,64), device=torch.device('cpu')):\n",
        "        super(Policy, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_frames = n_frames\n",
        "        self.conv1 = nn.Conv2d(n_frames, hidden_size, 7)\n",
        "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 5)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
        "        self.gs = transforms.Grayscale()\n",
        "        self.rs = transforms.Resize(img_size)\n",
        "        self.device = device\n",
        "\n",
        "    def preproc_state(self, state):\n",
        "        # State Preprocessing\n",
        "        state = state[:83,:].transpose(2,0,1) #Torch wants images in format (channels, height, width)\n",
        "        state = torch.from_numpy(state)\n",
        "        state = self.gs(state) # grayscale\n",
        "        state = self.rs(state) # resize\n",
        "        return state/255 # normalize\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutions\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        # Global Max Pooling\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.reshape(batch_size, self.hidden_size, -1).max(axis=2).values\n",
        "\n",
        "        # Layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def act(self, states, exploration=True):\n",
        "        # Stack 4 states\n",
        "        state = torch.vstack([self.preproc_state(state) for state in states]).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # Get Action Probabilities\n",
        "        probs = self.forward(state).cpu()\n",
        "\n",
        "        # Return Action and LogProb\n",
        "        action = probs.argmax(-1)\n",
        "        log_prob = None\n",
        "        if exploration:\n",
        "            m = Categorical(probs)\n",
        "            action = m.sample()\n",
        "            log_prob = m.log_prob(action)\n",
        "        return action.item(), log_prob\n",
        "\n",
        "    def to(self, device):\n",
        "        ret = super().to(device)\n",
        "        ret.device = device\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5kAeq1Rl1Hyj"
      },
      "outputs": [],
      "source": [
        "MAX_PATIENCE = 100 # Maximum consecutive steps with negative reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sYk1se-R3vmh"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, n_eval_episodes, policy):\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(n_eval_episodes):\n",
        "        state = env.reset() # state reset\n",
        "\n",
        "        # perform noop for 60 steps (noisy start)\n",
        "        for i in range(60):\n",
        "            state,_,_,_,_ = env.step(0)\n",
        "\n",
        "\n",
        "        done = False\n",
        "\n",
        "        # stats\n",
        "        total_rewards_ep = 0\n",
        "        negative_reward_patience = MAX_PATIENCE\n",
        "\n",
        "        # state\n",
        "        states = deque(maxlen=4)\n",
        "        for i in range(n_frames):\n",
        "            states.append(state)\n",
        "\n",
        "        while not done:\n",
        "            # perform action\n",
        "            action, _ = policy.act(states, exploration=False)\n",
        "\n",
        "            # As in practical 4, we do not consider \"truncated\" (i.e., reaching the max number of steps)\n",
        "            # as a termination condition\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "            states.append(state)\n",
        "\n",
        "            # handle patience\n",
        "            if reward >=0:\n",
        "                negative_reward_patience = MAX_PATIENCE\n",
        "            else:\n",
        "                negative_reward_patience -= 1\n",
        "                if negative_reward_patience == 0:\n",
        "                    done = True\n",
        "            if done: reward = -100\n",
        "\n",
        "            # stats\n",
        "            total_rewards_ep += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # stats\n",
        "        episode_rewards.append(total_rewards_ep)\n",
        "\n",
        "    # stats\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "\n",
        "    return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Uzb4bInRxMsx"
      },
      "outputs": [],
      "source": [
        "def reinforce(policy, optimizer, n_training_episodes=1000, gamma=0.99, print_every=5):\n",
        "    # stats\n",
        "    scores_deque = deque(maxlen=100)\n",
        "\n",
        "    for i_episode in range(1, n_training_episodes+1):\n",
        "        saved_log_probs = [] # stores log probs during episode\n",
        "        rewards = [] # stores rewards during episode\n",
        "\n",
        "        # init episode\n",
        "        state = env.reset()\n",
        "        for i in range(60):\n",
        "            state,_,_,_,_ = env.step(0)\n",
        "        done = False\n",
        "\n",
        "        negative_reward_patience = MAX_PATIENCE\n",
        "        states = deque(maxlen=4)\n",
        "        for i in range(n_frames):\n",
        "            states.append(state)\n",
        "\n",
        "\n",
        "        while not done:\n",
        "            action, log_prob = policy.act(states)\n",
        "\n",
        "            # store log_prob\n",
        "            ...\n",
        "\n",
        "            # As in practical 4, we do not consider \"truncated\" (i.e., reaching the max number of steps)\n",
        "            # as a termination condition\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            states.append(state)\n",
        "\n",
        "            if reward >=0:\n",
        "                negative_reward_patience = MAX_PATIENCE\n",
        "            else:\n",
        "                negative_reward_patience -= 1\n",
        "                if negative_reward_patience == 0:\n",
        "                    done = True\n",
        "            if done: reward = -100\n",
        "\n",
        "            # store reward\n",
        "            ...\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        scores_deque.append(sum(rewards))\n",
        "\n",
        "\n",
        "        rewards = np.array(rewards)\n",
        "        discounts = np.power(gamma, np.arange(len(rewards)))\n",
        "\n",
        "        policy_loss = 0\n",
        "        for t in range(len(rewards)):\n",
        "            G = ... # Return from timestep t\n",
        "            policy_loss += ... # loss for timestep t\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i_episode % print_every == 0:\n",
        "            print(f'''Episode {i_episode}\n",
        "                    \\tAverage Score: {np.mean(scores_deque)}\n",
        "                    \\tLast Score: {rewards.sum()}\n",
        "                    \\tEval Score: {evaluate_agent(eval_env,5,policy)}''')\n",
        "            torch.save(policy, 'model.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GwVv1ETxN7c6"
      },
      "outputs": [],
      "source": [
        "policy = Policy(n_frames, n_actions, 32).to(device)\n",
        "policy = policy.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bQYqOdbiy0ez"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(policy.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esS1pRU6D9CS"
      },
      "outputs": [],
      "source": [
        "reinforce(policy, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85P7ChOqN7c8",
        "outputId": "507d3997-1d9a-44cd-a59d-755b5373487d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy.device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTqv8LscQrgn"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLb0dedAQtx4",
        "outputId": "cfbb48a5-c355-4cc0-dceb-90812bfc65e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Policy(\n",
              "  (conv1): Conv2d(4, 32, kernel_size=(7, 7), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=5, bias=True)\n",
              "  (gs): Grayscale(num_output_channels=1)\n",
              "  (rs): Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy = torch.load('model.pt', map_location=device)\n",
        "policy = policy.to(device)\n",
        "policy.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cq28HU1UTEBo"
      },
      "outputs": [],
      "source": [
        "def play_agent(env, policy):\n",
        "    total_reward = 0\n",
        "    state = env.reset()\n",
        "    for i in range(60):\n",
        "        state,_,_,_,_ = env.step(0)\n",
        "    step = 0\n",
        "    done = False\n",
        "    negative_reward_patience = MAX_PATIENCE\n",
        "    states = deque(maxlen=4)\n",
        "    for i in range(policy.n_frames):\n",
        "        states.append(state)\n",
        "    while not done:\n",
        "        action, _ = policy.act(states, exploration=False)\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "        states.append(new_state)\n",
        "        if reward >=0:\n",
        "            negative_reward_patience = MAX_PATIENCE\n",
        "        else:\n",
        "            negative_reward_patience -= 1\n",
        "            if negative_reward_patience == 0:\n",
        "                done = True\n",
        "        if done:\n",
        "            reward = -100\n",
        "        total_reward += reward\n",
        "        env.render()\n",
        "        if done:\n",
        "            break\n",
        "        state = new_state\n",
        "    print(\"Total Reward:\", total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRWzLGZVTMiV",
        "outputId": "823268d1-5b75-413b-aab3-8341e8239700"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Reward: -56.131506849315265\n"
          ]
        }
      ],
      "source": [
        "play_agent(eval_env, policy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
