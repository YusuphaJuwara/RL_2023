{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pwCaZX3mf9O",
        "outputId": "de960485-c966-4051-9c49-7fb761403c89"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.11.1)' due to a timeout waiting for the ports to get used. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-mwORTUlRLF"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.11.1)' due to a timeout waiting for the ports to get used. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import namedtuple, deque\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoeUO3zWlRLJ"
      },
      "source": [
        "# Definition of a 3-layer neural net with tanh activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "19xScikUlRLL"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mg:\\My Drive\\AI_Robotics\\y1-s1-reinforcement-learning\\RL_2023\\p05_DDQN\\DDQN.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/My%20Drive/AI_Robotics/y1-s1-reinforcement-learning/RL_2023/p05_DDQN/DDQN.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/AI_Robotics/y1-s1-reinforcement-learning/RL_2023/p05_DDQN/DDQN.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNet\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/AI_Robotics/y1-s1-reinforcement-learning/RL_2023/p05_DDQN/DDQN.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, n_inputs, n_outputs, bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
        "        super().__init__()\n",
        "        self.activation_function= nn.Tanh()\n",
        "\n",
        "        self.layer1 = nn.Linear( #<--- linear layer\n",
        "            n_inputs, #<----------------#input features\n",
        "            64,#<-----------------------#output features\n",
        "            bias=bias)#<----------------bias\n",
        "\n",
        "        self.layer2 = nn.Linear(\n",
        "            64,\n",
        "            32,\n",
        "            bias=bias)\n",
        "\n",
        "        self.layer3 = nn.Linear(\n",
        "                    32,\n",
        "                    n_outputs,\n",
        "                    bias=bias)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation_function( self.layer1(x) )\n",
        "        x = self.activation_function( self.layer2(x) )\n",
        "        y = self.layer3(x)\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_eLFU0blRLM"
      },
      "source": [
        "# Q network definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ39zCL-lRLM"
      },
      "outputs": [],
      "source": [
        "class Q_network(nn.Module):\n",
        "\n",
        "    def __init__(self, env,  learning_rate=1e-4):\n",
        "        super(Q_network, self).__init__()\n",
        "\n",
        "        n_outputs = env.action_space.n\n",
        "        state = env.\n",
        "\n",
        "        # TODO\n",
        "        # self.network = Net( ?? , ??)\n",
        "        # NOTE: Network receives as input a state and must\n",
        "        # output a value for each action.\n",
        "        self.network = Net(1, 1)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(),\n",
        "                                          lr=learning_rate)\n",
        "\n",
        "    def greedy_action(self, state):\n",
        "        # TODO\n",
        "        # greedy action = ??\n",
        "        greedy_a = 0\n",
        "        return greedy_a\n",
        "\n",
        "    def get_qvals(self, state):\n",
        "        # TODO\n",
        "        #qval = ???\n",
        "        qval = 0\n",
        "        return qval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gharOAZ3lRLN"
      },
      "source": [
        "## Experience replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99zdRJ3ClRLN"
      },
      "outputs": [],
      "source": [
        "class Experience_replay_buffer:\n",
        "\n",
        "    def __init__(self, memory_size=50000, burn_in=10000):\n",
        "        self.memory_size = memory_size\n",
        "        self.burn_in = burn_in\n",
        "        self.Buffer = namedtuple('Buffer',\n",
        "                                 field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
        "        self.replay_memory = deque(maxlen=memory_size)\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        samples = np.random.choice(len(self.replay_memory), batch_size,\n",
        "                                   replace=False)\n",
        "        # Use asterisk operator to unpack deque\n",
        "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
        "        return batch\n",
        "\n",
        "    def append(self, s_0, a, r, d, s_1):\n",
        "        self.replay_memory.append(\n",
        "            self.Buffer(s_0, a, r, d, s_1))\n",
        "\n",
        "    def burn_in_capacity(self):\n",
        "        return len(self.replay_memory) / self.burn_in\n",
        "\n",
        "    def capacity(self):\n",
        "        return len(self.replay_memory) / self.memory_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5TEby1LlRLN"
      },
      "source": [
        "# DDQN agent implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUXA2lWOlRLO"
      },
      "outputs": [],
      "source": [
        "def from_tuple_to_tensor(tuple_of_np):\n",
        "    tensor = torch.zeros((len(tuple_of_np), tuple_of_np[0].shape[0]))\n",
        "    for i, x in enumerate(tuple_of_np):\n",
        "        tensor[i] = torch.FloatTensor(x)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "class DDQN_agent:\n",
        "\n",
        "    def __init__(self, env, rew_thre, buffer, learning_rate=0.001, initial_epsilon=0.5, batch_size= 64):\n",
        "\n",
        "        self.env = env\n",
        "\n",
        "\n",
        "        self.network = Q_network(env, learning_rate).to(device)\n",
        "        # TODO\n",
        "        #self.target_network = ???\n",
        "        self.buffer = buffer\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.window = 50\n",
        "        self.reward_threshold = rew_thre\n",
        "        self.initialize()\n",
        "        self.step_count = 0\n",
        "        self.episode = 0\n",
        "\n",
        "\n",
        "    def take_step(self, mode='exploit'):\n",
        "        # choose action with epsilon greedy\n",
        "        if mode == 'explore':\n",
        "                action = self.env.action_space.sample()\n",
        "        else:\n",
        "                action = self.network.greedy_action(torch.FloatTensor(self.s_0).to(device))\n",
        "\n",
        "        #simulate action\n",
        "        s_1, r, terminated, truncated, _ = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        #put experience in the buffer\n",
        "        # TODO\n",
        "        #self.buffer ???\n",
        "\n",
        "        self.rewards += r\n",
        "\n",
        "        self.s_0 = s_1.copy()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if done:\n",
        "            self.s_0, _ = self.env.reset()\n",
        "        return done\n",
        "\n",
        "    # Implement DQN training algorithm\n",
        "    def train(self, gamma=0.99, max_episodes=10000,\n",
        "              network_update_frequency=10,\n",
        "              network_sync_frequency=200):\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.s_0, _ = self.env.reset()\n",
        "\n",
        "        # Populate replay buffer\n",
        "        while self.buffer.burn_in_capacity() < 1:\n",
        "            self.take_step(mode='explore')\n",
        "        ep = 0\n",
        "        training = True\n",
        "        self.populate = False\n",
        "        while training:\n",
        "            self.s_0, _ = self.env.reset()\n",
        "\n",
        "            self.rewards = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                if ((ep % 5) == 0):\n",
        "                    self.env.render()\n",
        "\n",
        "                p = np.random.random()\n",
        "                if p < self.epsilon:\n",
        "                    done = self.take_step(mode='explore')\n",
        "                    # print(\"explore\")\n",
        "                else:\n",
        "                    done = self.take_step(mode='exploit')\n",
        "                    # print(\"train\")\n",
        "                # Update network\n",
        "                if self.step_count % network_update_frequency == 0:\n",
        "                    self.update()\n",
        "                # Sync networks\n",
        "                if self.step_count % network_sync_frequency == 0:\n",
        "                    # TODO: synchronize Qnet and target_net\n",
        "                    #self.target_network ???\n",
        "                    self.sync_eps.append(ep)\n",
        "\n",
        "                if done:\n",
        "                    if self.epsilon >= 0.05:\n",
        "                        self.epsilon = self.epsilon * 0.7\n",
        "                    ep += 1\n",
        "                    if self.rewards > 2000:\n",
        "                        self.training_rewards.append(2000)\n",
        "                    elif self.rewards > 1000:\n",
        "                        self.training_rewards.append(1000)\n",
        "                    elif self.rewards > 500:\n",
        "                        self.training_rewards.append(500)\n",
        "                    else:\n",
        "                        self.training_rewards.append(self.rewards)\n",
        "                    if len(self.update_loss) == 0:\n",
        "                        self.training_loss.append(0)\n",
        "                    else:\n",
        "                        self.training_loss.append(np.mean(self.update_loss))\n",
        "                    self.update_loss = []\n",
        "                    mean_rewards = np.mean(self.training_rewards[-self.window:])\n",
        "                    mean_loss = np.mean(self.training_loss[-self.window:])\n",
        "                    self.mean_training_rewards.append(mean_rewards)\n",
        "                    print(\n",
        "                        \"\\rEpisode {:d} Mean Rewards {:.2f}  Episode reward = {:.2f}   mean loss = {:.2f}\\t\\t\".format(\n",
        "                            ep, mean_rewards, self.rewards, mean_loss), end=\"\")\n",
        "\n",
        "                    if ep >= max_episodes:\n",
        "                        training = False\n",
        "                        print('\\nEpisode limit reached.')\n",
        "                        break\n",
        "                    if mean_rewards >= self.reward_threshold:\n",
        "                        training = False\n",
        "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
        "                            ep))\n",
        "                        #break\n",
        "        # save models\n",
        "        self.save_models()\n",
        "        # plot\n",
        "        self.plot_training_rewards()\n",
        "\n",
        "    def save_models(self):\n",
        "        torch.save(self.network, \"Q_net\")\n",
        "\n",
        "    def load_models(self):\n",
        "        self.network = torch.load(\"Q_net\")\n",
        "        self.network.eval()\n",
        "\n",
        "    def plot_training_rewards(self):\n",
        "        plt.plot(self.mean_training_rewards)\n",
        "        plt.title('Mean training rewards')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.xlabel('Episods')\n",
        "        plt.show()\n",
        "        plt.savefig('mean_training_rewards.png')\n",
        "        plt.clf()\n",
        "\n",
        "    def calculate_loss(self, batch):\n",
        "        #extract info from batch\n",
        "        states, actions, rewards, dones, next_states = list(batch)\n",
        "\n",
        "        #transform in torch tensors\n",
        "        rewards = torch.FloatTensor(rewards).reshape(-1, 1).to(device)\n",
        "        actions = torch.LongTensor(np.array(actions)).reshape(-1, 1).to(device)\n",
        "        dones = torch.IntTensor(dones).reshape(-1, 1).to(device)\n",
        "        states = from_tuple_to_tensor(states).to(device)\n",
        "        next_states = from_tuple_to_tensor(next_states).to(device)\n",
        "\n",
        "        ###############\n",
        "        # DDQN Update #\n",
        "        ###############\n",
        "        # TODO\n",
        "        # Q(s,a) = ??\n",
        "        # Hint! You can use torch.gather to select the qvals\n",
        "        # corresponding to the actions in the batch.\n",
        "        #\n",
        "        #\n",
        "\n",
        "        # TODO\n",
        "        # target Q(s,a) = ??\n",
        "        #\n",
        "        #\n",
        "        #\n",
        "\n",
        "\n",
        "        # TODO\n",
        "        #loss = self.loss_function( Q(s,a), target_Q(s,a))\n",
        "        loss = 0\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def update(self):\n",
        "        self.network.optimizer.zero_grad()\n",
        "        batch = self.buffer.sample_batch(batch_size=self.batch_size)\n",
        "        loss = self.calculate_loss(batch)\n",
        "\n",
        "        loss.backward()\n",
        "        self.network.optimizer.step()\n",
        "\n",
        "        self.update_loss.append(loss.item())\n",
        "\n",
        "    def initialize(self):\n",
        "        self.training_rewards = []\n",
        "        self.training_loss = []\n",
        "        self.update_loss = []\n",
        "        self.mean_training_rewards = []\n",
        "        self.sync_eps = []\n",
        "        self.rewards = 0\n",
        "        self.step_count = 0\n",
        "\n",
        "    def evaluate(self, eval_env):\n",
        "        done = False\n",
        "        s, _ = eval_env.reset()\n",
        "        rew = 0\n",
        "        while not done:\n",
        "            action = self.network.greedy_action(torch.FloatTensor(s).to(device))\n",
        "            s, r, terminated, truncated, _ = eval_env.step(action)\n",
        "            done = terminated or truncated\n",
        "            rew += r\n",
        "\n",
        "        print(\"Evaluation cumulative reward: \", rew)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAk-zkkblRLP"
      },
      "source": [
        "# Train and evaluate on cartpole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoZXxNoilRLQ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "rew_threshold = 400\n",
        "buffer = Experience_replay_buffer()\n",
        "agent = DDQN_agent(env, rew_threshold, buffer)\n",
        "agent.train()\n",
        "\n",
        "eval_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "agent.evaluate(eval_env)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
